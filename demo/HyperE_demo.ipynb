{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperE_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/huaijun-lee/hyperE/blob/master/demo/HyperE_demo.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ah622QlHnZ0r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "from scipy.sparse.csgraph import floyd_warshall, connected_components\n",
        "import operator\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import json\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YD8Ju0ZQnZ0v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some definitions\n",
        "\n",
        "# Reflection (circle inversion of x through orthogonal circle centered at a)\n",
        "def isometric_transform(a, x):\n",
        "    r2 = np.linalg.norm(a)**2 - (1.0)\n",
        "    return r2/np.linalg.norm(x - a)**2 * (x-a) + a\n",
        "\n",
        "# Inversion taking mu to origin\n",
        "def reflect_at_zero(mu,x):\n",
        "    a = mu/np.linalg.norm(mu)**2\n",
        "    return isometric_transform(a,x)\n",
        "\n",
        "def acosh(x):\n",
        "    return np.log(x + np.sqrt(x**2-1))\n",
        "\n",
        "# Hyperbolic distance\n",
        "def dist(u,v):\n",
        "    z  = 2 * np.linalg.norm(u-v)**2\n",
        "    uu = 1. + z/((1-np.linalg.norm(u)**2)*(1-np.linalg.norm(v)**2))\n",
        "    return acosh(uu)\n",
        "\n",
        "# Hyperbolic distance from 0\n",
        "def hyp_dist_origin(x):\n",
        "    return np.log((1+np.linalg.norm(x))/(1-np.linalg.norm(x)))\n",
        "\n",
        "# Scalar multiplication w*x\n",
        "def hyp_scale(w, x):\n",
        "    sgn = (-1.0)**float(w<0)\n",
        "    w *= sgn    \n",
        "    if w == 1:\n",
        "        return sgn*x\n",
        "    else:\n",
        "        x_dist = (1+np.linalg.norm(x))/(1-np.linalg.norm(x))\n",
        "        alpha = 1-2/(1+x_dist**w)\n",
        "        alpha *= 1/np.linalg.norm(x)\n",
        "    \n",
        "    return sgn*alpha*x\n",
        "\n",
        "# Convex combination (1-w)*x+w*y\n",
        "def hyp_conv_comb(w, x, y):\n",
        "    # circle inversion sending x to 0\n",
        "    (xinv, yinv) = (reflect_at_zero(x, x), reflect_at_zero(x, y))\n",
        "    # scale by w \n",
        "    pinv = hyp_scale(w, yinv)\n",
        "    # reflect back\n",
        "    return reflect_at_zero(x, pinv)\n",
        "\n",
        "# Weighted sum w1*x + w2*y\n",
        "def hyp_weighted_sum(w1, w2, x, y):\n",
        "    p = hyp_conv_comb(w2 / (w1 + w2), x, y)\n",
        "    return hyp_scale(w1 + w2, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsW-lMBSnZ0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        },
        "outputId": "2b8f30c5-e268-4930-e878-8a56e14343aa"
      },
      "cell_type": "code",
      "source": [
        "# Creating the edge list for hypernym relationship in Wordnet.\n",
        "\n",
        "def load_wordnet():\n",
        "    SynstoIDs  = dict()\n",
        "    IDstoSyns = dict()\n",
        "    all_syns = list(wn.all_synsets())\n",
        "    \n",
        "    for idx, x in enumerate(all_syns):\n",
        "        SynstoIDs[x] = idx\n",
        "        IDstoSyns[idx] = x\n",
        "\n",
        "    n = len(all_syns)\n",
        "    e = make_edge_set()\n",
        "\n",
        "    for idx, x in enumerate(all_syns):\n",
        "        for y in x.hypernyms():\n",
        "            y_idx = SynstoIDs[y]\n",
        "            add_edge(e, idx  , y_idx)\n",
        "            add_edge(e, y_idx,   idx)\n",
        "            \n",
        "    X = csr_matrix(e, shape=(n, n))\n",
        "\n",
        "    return SynstoIDs, IDstoSyns, X, all_syns\n",
        "\n",
        "    \n",
        "SynstoIDs, IDstoSyns, X, all_syns = load_wordnet()\n",
        "G = nx.from_scipy_sparse_matrix(X)\n",
        "Gc = max(nx.connected_component_subgraphs(G), key=len)\n",
        "\n",
        "# reorder with nx\n",
        "Gc_final = nx.convert_node_labels_to_integers(Gc, ordering=\"decreasing degree\", label_attribute=\"old_label\")\n",
        "\n",
        "#Create the dict for old-id <-> new-id matching for syns\n",
        "RefDict = Gc_final.node\n",
        "IDsToSyns_f = dict()\n",
        "SynsToIDs_f = dict()\n",
        "for new_idx in RefDict.keys():\n",
        "    old_idx = RefDict[new_idx]['old_label']\n",
        "    curr_syn = IDstoSyns[old_idx]\n",
        "    IDsToSyns_f[new_idx] = curr_syn\n",
        "    SynsToIDs_f[curr_syn] = new_idx\n",
        "    \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0653809a6949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mSynstoIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDstoSyns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_syns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wordnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_scipy_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mGc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected_component_subgraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0653809a6949>\u001b[0m in \u001b[0;36mload_wordnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSynstoIDs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mIDstoSyns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mall_syns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_syns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aGNlJQZ9nZ0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read the emb files, save their tau and emb_dict.\n",
        "\n",
        "emb_files = {\n",
        "            'hypernym_demo.emb':'hypernym_demo',\n",
        "            }\n",
        "\n",
        "RelEmbDict = defaultdict(dict)\n",
        "RelTauDict = defaultdict(dict)\n",
        "for file in emb_files.keys():\n",
        "    with open(file, 'r') as emb:\n",
        "        emb_lines = emb.readlines()    \n",
        "    emb_lines = emb_lines[1:]\n",
        "    emb_dict = dict()\n",
        "    rel = emb_files[file]\n",
        "    for idx, line in enumerate(emb_lines):\n",
        "        curr_line = line.split(',')\n",
        "        curr_tau = curr_line[-1].split(\"\\n\")[0]\n",
        "        curr_tau = np.float64(curr_tau)\n",
        "        curr_line = curr_line[:-1]\n",
        "        curr_idx = int(curr_line[0])                \n",
        "        emb_dict[curr_idx] = np.asarray(list(map(np.float64, curr_line[1:])))\n",
        "    RelEmbDict[rel] = emb_dict\n",
        "    RelTauDict[rel] = curr_tau\n",
        "    \n",
        "\n",
        "#Construct W matrices for each relationship separately. \n",
        "    \n",
        "vector_dim = 10\n",
        "ReltoW = defaultdict()\n",
        "for rel in RelEmbDict.keys():\n",
        "    emb_dict_curr = RelEmbDict[rel]\n",
        "    vocab_size = len(emb_dict_curr)\n",
        "    W_curr = np.zeros((vocab_size, vector_dim))\n",
        "    for idx, vec in emb_dict_curr.items():\n",
        "        W_curr[idx,:] = vec\n",
        "    ReltoW[rel] = W_curr\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6BrmwVnnZ01",
        "colab_type": "code",
        "colab": {},
        "outputId": "5674813d-bf9a-4945-f30b-ab68139be8de"
      },
      "cell_type": "code",
      "source": [
        "# Find the top 10 nearest neighbors to a particular synset for given relationship.\n",
        "\n",
        "vector_dim = 10\n",
        "rel = 'hypernym_demo'\n",
        "emb_dict_curr = RelEmbDict[rel]\n",
        "vocab_size = len(emb_dict_curr)\n",
        "W = np.zeros((vocab_size, vector_dim))\n",
        "relTau = RelTauDict[rel]\n",
        "\n",
        "\n",
        "e1 = wn.synset('geometry.n.01')\n",
        "e1_idx = SynsToIDs_f[e1]\n",
        "\n",
        "\n",
        "for idx, vec in emb_dict_curr.items():\n",
        "    W[idx,:] = vec\n",
        "    \n",
        "vec_e1 = emb_dict_curr[e1_idx] \n",
        "curr_dist = []    \n",
        "for row_idx in range(W.shape[0]):\n",
        "    curr_vec = W[row_idx,:]\n",
        "    normalized_dist = (dist(curr_vec,vec_e1))/relTau\n",
        "    curr_dist.append(normalized_dist)\n",
        "\n",
        "\n",
        "curr_dist[e1_idx] = np.Inf\n",
        "curr_closest_indices = np.argsort(curr_dist)[:10]\n",
        "for r_idx in curr_closest_indices:\n",
        "    relev_syn = IDsToSyns_f[r_idx]\n",
        "    print(curr_dist[r_idx], relev_syn.name(), relev_syn.definition())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.999999957925 non-euclidean_geometry.n.01 (mathematics) geometry based on axioms different from Euclid's\n",
            "0.999999990071 plane_geometry.n.01 the geometry of 2-dimensional figures\n",
            "0.999999994248 solid_geometry.n.01 the geometry of 3-dimensional space\n",
            "0.999999994509 fractal_geometry.n.01 (mathematics) the geometry of fractals\n",
            "1.00000000061 pure_mathematics.n.01 the branches of mathematics that study and develop the principles of mathematics for their own sake rather than for their immediate usefulness\n",
            "1.00000000189 spherical_geometry.n.01 (mathematics) the geometry of figures on the surface of a sphere\n",
            "1.00000000937 projective_geometry.n.01 the geometry of properties that remain invariant under projection\n",
            "1.0000000101 analytic_geometry.n.01 the use of algebra to study geometric properties; operates on symbols defined in a coordinate system\n",
            "1.00000001566 affine_geometry.n.01 the geometry of affine transformations\n",
            "1.00000002831 elementary_geometry.n.01 (mathematics) geometry based on Euclid's axioms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G-dyVw59nZ05",
        "colab_type": "code",
        "colab": {},
        "outputId": "da781ba9-12f3-48b0-fafb-10175751e5df"
      },
      "cell_type": "code",
      "source": [
        "# Word analogy for selected examples.\n",
        "\n",
        "vector_dim = 10\n",
        "rel = 'hypernym_demo'\n",
        "emb_dict_curr = RelEmbDict[rel]\n",
        "vocab_size = len(emb_dict_curr)\n",
        "W = np.zeros((vocab_size, vector_dim))\n",
        "relTau = RelTauDict[rel]\n",
        "\n",
        "\n",
        "# Choose the entities.\n",
        "e1 = wn.synset('guitarist.n.01')\n",
        "e1_idx = SynsToIDs_f[e1]\n",
        "\n",
        "e2 = wn.synset('musician.n.01')\n",
        "e2_idx = SynsToIDs_f[e2]\n",
        "\n",
        "e3 = wn.synset('novelist.n.01')\n",
        "e3_idx = SynsToIDs_f[e3]\n",
        "\n",
        "\n",
        "for idx, vec in emb_dict_curr.items():\n",
        "    W[idx,:] = vec\n",
        "    \n",
        "vec_e1 = emb_dict_curr[e1_idx]\n",
        "vec_e2 = emb_dict_curr[e2_idx]\n",
        "vec_e3 = emb_dict_curr[e3_idx]\n",
        "\n",
        "\n",
        "vec1_ = hyp_scale(-1, vec_e1)\n",
        "left_sum = hyp_weighted_sum(1, 1, vec_e2, vec1_)\n",
        "vec_search = hyp_weighted_sum(1, 1, left_sum, vec_e3)\n",
        "\n",
        "curr_dist = []    \n",
        "for row_idx in range(W.shape[0]):\n",
        "    curr_vec = W[row_idx,:]\n",
        "    normalized_dist = (dist(curr_vec, vec_search))/relTau\n",
        "    curr_dist.append(normalized_dist)\n",
        "\n",
        "\n",
        "curr_dist[e1_idx] = np.Inf\n",
        "curr_dist[e2_idx] = np.Inf\n",
        "curr_dist[e3_idx] = np.Inf\n",
        "\n",
        "curr_closest_indices = np.argsort(curr_dist)[:10]\n",
        "for r_idx in curr_closest_indices:\n",
        "    relev_syn = IDsToSyns_f[r_idx]\n",
        "    print(curr_dist[r_idx], relev_syn.name(), relev_syn.definition())\n",
        "\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.716374331939 writer.n.01 writes (books or stories or articles or the like) professionally (for pay)\n",
            "0.987304286504 communicator.n.01 a person who communicates with others\n",
            "1.17175459928 librettist.n.01 author of words to be set to music in an opera or operetta\n",
            "1.18552618387 announcer.n.01 someone who proclaims a message publicly\n",
            "1.18607557487 essayist.n.01 a writer of literary works\n",
            "1.20045196565 commentator.n.02 a writer who reports and analyzes events of the day\n",
            "1.21288127055 gagman.n.02 someone who writes comic material for public performers\n",
            "1.21336310414 sympathizer.n.01 commiserates with someone who has had misfortune\n",
            "1.25372619738 lyricist.n.01 a person who writes the words for songs\n",
            "1.26142195216 alliterator.n.01 a speaker or writer who makes use of alliteration\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}